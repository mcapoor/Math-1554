% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[12pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{parskip}
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float

% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\usepackage{amsmath}
\usepackage{amssymb}
%%% END Article customizations

\newcommand{\R}{\mathbb{R}}
\newcommand{\mateq}{$A \vec{x} = \vec{b}$}
%%% The "real" document content comes below...
\pagenumbering{arabic}

\graphicspath{{./images/}}

\title{Math 1554}
\author{Milan Capoor}
\date{Fall 2021} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Module 1: Linear Equations}
\subsection{TOPIC 1: Systems of Linear Equations}
\emph{Linear equation:} an equation in the form $a_1 x_1 + a_2 x_2 + ... + a_n x_n = b$ where $a_n$ and $b$ are real or complex coefficients of the variables $x_n$\\

\emph{Linear system (system of linear equations):} a collection of linear equations involving the same variables\\

\emph{Solution:} a list of values ($s_1, s_2, ..., s_n$) for which substitution into the variables 
($x_1, x_2, ... , x_n$) makes the equations true\\
\indent - Found where the equationsâ€™ lines intersect \\

\emph{Solution set:} the collection of all solutions to a linear system\\

\emph{Equivalent:} characteristic of two linear systems if they have the same solution sets\\

\emph{Consistent system:} a linear system which has infinitely many solutions or one solution\\
\indent - Happens when the lines intersect at a single point or when the lines coincide\\

\emph{Inconsistent system:} system with no solutions\\
\indent - When the lines are parallel\\



\emph{Matrix:} the essential information of a linear system can be recorded compactly in a rectangular array\\
Given the system
\begin{align*}
x_1 - 2x_2 + x_3 &= 0\\
\;	2x_2 - 8x_3 &= 8\\
5x_1\; 	- 5x_3 &= 10\\
\end{align*}
The \emph{coefficient matrix}
\begin{center}
	$\begin{bmatrix}
		1 & -2 & 1\\ 
		0 & 2 & -8 \\
		5 & 0 & -5\\
	\end{bmatrix}$
\end{center}
encodes the essential information of the system while the \emph{augmented matrix} is \\
\begin{center}
	$\begin{bmatrix}
		1 & -2 & 1 & 0\\ 
		0 & 2 & -8 & 8\\
		5 & 0 & -5 & 10\\
	 \end{bmatrix}$
\end{center}

An $m \times n$ matrix is a rectangular array of numbers with $m$ rows and $n$ columns.

\subsubsection{Solving a linear system}
The basic strategy is to \emph{replace one system with an equivalent system that is easier to solve}. Three basic operations are used to simplify a linear system: 
\begin{enumerate}
\item (Replacement) Replace one equation by the sum of itself and a multiple of another equation 
\item (Interchange) Swap two equations
\item (Scaling) Multiply all the terms in an equation by a nonzero constant
\end{enumerate}

\subsubsection{Example 1: Solve the system given above}
\begin{align*}
	x_1 - 2x_2 + x_3 &= 0\\
	\;	2x_2 - 8x_3 &= 8\\
	5x_1\; 	- 5x_3 &= 10\\
\end{align*}

\indent Row Reduction Procedure:
\begin{enumerate}
	\item Construct the augmented matrix
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 1 & 0\\ 
				0 & 2 & -8 & 8\\
				5 & 0 & -5 & 10\\
			 \end{bmatrix}$
		\end{center}
	\item $R_3 - 5R_1$
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 1 & 0\\ 
				0 & 2 & -8 & 8\\
				0 & 10 & -10 & 10\\
			 \end{bmatrix}$
		\end{center}
	\item $\frac{1}{2} R_2$
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 1 & 0\\ 
				0 & 1 & -4 & 4\\
				5 & 0 & -5 & 10\\
			 \end{bmatrix}$
		\end{center}
	\item $R_3 - 10R_2$
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 1 & 0\\ 
				0 & 1 & -4 & 4\\
				0 & 0 & 30 & -30\\
			 \end{bmatrix}$
		\end{center}
	\item $\frac{1}{30}R_3$
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 1 & 0\\ 
				0 & 1 & -4 & 4\\
				0 & 0 & 1 & -1\\
			 \end{bmatrix}$
		\end{center}
	\item $R_1 - R_3 and R_2 + 4R_3$
		\begin{center}
			$\begin{bmatrix}
				1 & -2 & 0 & 1\\ 
				0 & 1 & 0 & 0\\
				0 & 0 & 1 & -1\\
			 \end{bmatrix}$
		\end{center}
	\leavevmode \\
	\item $R_1 + 2R_2$
		\begin{center}
			$\begin{bmatrix}
				1 & 0 & 0 & 1\\ 
				0 & 1 & 0 & 0\\
				0 & 0 & 1 & -1\\
			 \end{bmatrix}$
		\end{center}
	\item Verify the solution $(1, 0, -1)$ by substituting it for $(x_1, x_2, x_3)$ in the original system
		\begin{align*}
			(1) - 2(0) + (-1) &= 0\\
			2(0) - 8(-1) &= 8\\
			5(1) - 5(-1) &= 10\\
		\end{align*}
\end{enumerate}
The equations agree so $(1, 0, -1)$ is indeed a solution to the system. $\blacksquare$\\

These row operations can be applied to any matrix \textemdash not just those that arise as the augmented matrices of a linear system.\\

\emph{Row equivalent:} quality of two matrices if there exists a sequence of elementary row operations that transforms one matrix into the other\\

\textbf{All row operations are reversible. Hence, \emph{If the augmented matrices of two linear systems are row equivalent, then the two systems have the same solution set.}}

\subsubsection{Existence and Uniqueness}
A large focus of the course and of the analysis of linear systems generally depends on asking two questions:
\begin{itemize}
	\item Is the system consistent?
	\item If a solution exists, is it unique?
\end{itemize}
These questions can often be answered from the triangular form of the matrix (in Example 1 this was $\begin{bmatrix}1 & -2 & 1 & 0\\ 0 & 1 & -4 & 4\\0 & 0 & 1 & -1\\ \end{bmatrix}$) by understanding that $x_3 = -1$ and solving the other equations from there. \\
If this method creates contradictions, however, such as in the system $\begin{bmatrix}a & b & c & d\\ e & f & g & h\\0 & 0 & 0 & 1\\ \end{bmatrix}$, then the originial system is inconsistent. \\

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 2: Row Reduction and Echelon Forms}
\emph{Nonzero:} any row or column which contains at least one nonzero entry\\

\emph{Leading entry:} the leftmost nonzero entry of a row\\

\emph{Row echelon form:} a matrix is in row echelon form if it has the following three properties:
\begin{enumerate}
	\item All nonzero rows are above any rows of all zeros
	\item Each leading entry of a row is in a column to the right of the leading entry of the row above it
	\item All entries in a column below a leading entry are zeroes
\end{enumerate}
\emph{Reduced echelon form:} a matrix in row echelon form which satisfies the additional criteria:
\begin{enumerate}
\addtocounter{enumi}{3}
\item The leading entry in each nonzero row is 1
\item Each leading 1 is the only nonzero entry in its column
\end{enumerate}
These are the "triangular" matrices of section 1.1.\\

\textbf{\emph{Each matrix is row equivalent to one and only one reduced echelon matrix.}}\\

If a matrix $A$ is row equivalent to an echelon matrix $U$, we call $U$ an \emph{(reduced) echelon form (REF/RREF) of $A$}\\

\subsubsection{Pivot Positions}
\emph{Pivot position:} a location in a matrix $A$ that corresponds to a leading 1 in the reduced echelon form of $A$.\\

\emph{Pivot column:} a column of $A$ that contains a pivot position\\
\subsubsection{Example 2: Row reduce matrix A below to echelon form and locate its pivot columns.}
\begin{center}
	$A = \begin{bmatrix}
			0 & -3 & -6 & 4 & 9\\
			-1 & -2 & -1 & 3 & 1\\
			-2 & -3 & 0 & 3 & -1\\
			1 & 4 & 5 & -9 & -7\\
		\end{bmatrix}$
\end{center}
Solution:
\begin{enumerate}
\item Interchange $R_1$ and $R_4$
	\begin{center}
		$\begin{bmatrix}
			1 & 4 & 5 & -9 & -7\\
			-1 & -2 & -1 & 3 & 1\\
			-2 & -3 & 0 & 3 & -1\\
			0 & -3 & -6 & 4 & 9\\
		\end{bmatrix}$
	\end{center}
\item $R_2 + R_1$ and $R_3 + 2R_1$
	\begin{center}
		$\begin{bmatrix}
			1 & 4 & 5 & -9 & -7\\
			0 & 2 & 4 & -6 & -6\\
			0 & 5 & 10 & -15 & -15\\
			0 & -3 & -6 & 4 & 9\\
		\end{bmatrix}$
	\end{center}
\item $R_3 -\frac{5}{2}R_2$ and $R_4 + \frac{3}{2}R_2$
	\begin{center}
		$\begin{bmatrix}
			1 & 4 & 5 & -9 & -7\\
			0 & 2 & 4 & -6 & -6\\
			0 & 0 & 0 & 0 & 0\\
			0 & 0 & 0 & -5 & 0\\
		\end{bmatrix}$
	\end{center}
\leavevmode \\
\item Interchange $R_3$ and $R_4$
	\begin{center}
		$\begin{bmatrix}
			1 & 4 & 5 & -9 & -7\\
			0 & 2 & 4 & -6 & -6\\
			0 & 0 & 0 & -5 & 0\\
			0 & 0 & 0 & 0 & 0\\
		\end{bmatrix}$
	\end{center}	
\end{enumerate}
Matrix $A$ is thus in echelon form and so columns 1, 2, and 4 are pivot columns.\\

\emph{Pivot:} a nonzero number in a pivot position which is used during row reduction to create zeros\\

\subsubsection{The Row Reduction Algorithm}
\begin{enumerate}
	\item Begin with the leftmost nonzero column. This is a pivot column. The pivot position is at the top.
	\item Select a nonzero entry in the pivot column as a pivot. If necessary, interchange rows to move this entry into the pivot position. 
	\item Use row replacement operations to create zeros in all positions below the pivot.
	\item Cover the row containing the pivot position and all rows above it. Apply steps 1-3 to the remaining submatrix, iterating until there are no more nonzero rows to modify. At this time we have the row echelon form.
	\item This step is used to find the reduced row echelon form. Beginning with the righmost pivot and working upward and to the left, create zeros above each pivot. If a pivot is not 1, make it 1 via scaling operation.
\end{enumerate}
\leavevmode \\
\subsubsection{Solutions of Linear Systems}
The row reduction algorithm leads directly to the solution set of a system when the algorithm is applied to the augmented matrix of the system.\\

\emph{Basic variables:} the variables of a linear system corresponding to pivot columns in the reduced echelon matrix \\

\emph{Free variables:} the other variables in the system\\

Whenever a system is consistent, the solution set can be described explicitly by solving the reduced system of equations for the basic variables in terms of the free variables.\\
Each different choice of a free variable determines a (different) solution of the system and every solution of the system is determined by a choice of the free variables.

\subsubsection{Example 4: Find the general solution of the linear system whose augmented matrix has been reduced to}
\begin{center}
	$\begin{bmatrix}
		1 & 6 & 2 & -5 & -2 & -4\\
		0 & 0 & 2 & -8 & -1 & 3\\
		0 & 0 & 0 & 0 & 1 & 7
	\end{bmatrix}$
\end{center}
Moving from echelon form to reduced echelon form we have:\\

$\begin{bmatrix}
	1 & 6 & 2 & -5 & -2 & -4\\
	0 & 0 & 2 & -8 & -1 & 3\\
	0 & 0 & 0 & 0 & 1 & 7
\end{bmatrix}$
\textasciitilde 
$\begin{bmatrix}
	1 & 6 & 2 & -5 & 0 & 10\\
	0 & 0 & 2 & -8 & 0 & 10\\
	0 & 0 & 0 & 0 & 1 & 7
\end{bmatrix}$
\textasciitilde 
$\begin{bmatrix}
	1 & 6 & 2 & -5 & 0 & 10\\
	0 & 0 & 1 & -4 & 0 & 5\\
	0 & 0 & 0 & 0 & 1 & 7
\end{bmatrix}$
\textasciitilde
$\begin{bmatrix}
	1 & 6 & 0 & 3 & 0 & 0\\
	0 & 0 & 1 & -4 & 0 & 5\\
	0 & 0 & 0 & 0 & 1 & 7\\
\end{bmatrix}$



Which is equivalent to 
\begin{align*}
	x_1 + 6x_2 + 3x_4 &= 0\\
	x_3 - 4x_4 &= 5 \\
	x_5 &= 7\\
\end{align*}
Because the pivot columns are 1, 3, and 5, the basic variables are $x_1, x_3, and x_5$, leaving $x_2$ and $x_4$ as free variables. Thus
\begin{center}
	$\begin{cases}
		x_1 = -6x_2 - 3x_4\\
		x_2 \text{ is free}\\
		x_3 = 5 + 4x_4\\
		x_4 \text{ is free}\\
		x_5 = 7\\
	\end{cases}$
\end{center}
\subsubsection{Existence and Uniqueness Questions (Redux)}
When a system is in echelon form and contains no equation of the form $0 = b (b \neq 0)$, \emph{every} nonzero equation contains a basic variable with a nonzero coefficient. Either the basic variables are completely determined (no free variables), signifying a unique solution, or the basic variables can be expressed in terms of one or more free variables, signifying infinitely many solutions. Thus:\\
\textbf{A linear system is consistent iff the rightmost column of the augmented matrix is \emph{not} a pivot column (i.e. there are no rows of the form $\begin{bmatrix} 0 & ... & 0 & b\end{bmatrix}$).}


%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 3: Vector Equations}
\emph{Column vector:} a matrix with only one column
$$\mathbf{w}= \begin{bmatrix}w_1 \\ w_2 \end{bmatrix}$$
The set of all vectors with two entries is denoted $\mathbb{R}^2$ ("r-two"), where $\mathbb{R}$ refers to the real number entries of the vector and the exponent 2 indicates that each vector contains two entries.\\
Vectors in $\mathbb{R}$ are ordered pairs which are equal iff their corresponding entries are equal.\\

Given two vectors \textbf{u} and \textbf{v} in $\R^2$, their sum is 
\begin{center}\textbf{u} + \textbf{v} = $\begin{bmatrix}u_1 + v_1 \\ u_2 + v_2\end{bmatrix}$\end{center}
while the scalar multiple of a vector is 
\begin{center}c\textbf{u} = c$\begin{bmatrix}u_1 \\ u_2\end{bmatrix} =\begin{bmatrix}c\cdot u_1 \\ c\cdot u_2\end{bmatrix}  $\end{center}

\subsubsection{Geometric Descriptions of $\R^2$}
Because each point in the cartesian plane is determined by an ordered pair of numbers, we can identify a geometric point with the column vector $\begin{bmatrix}a \\ b\end{bmatrix}$. So we may regard $\R^2$ as the set of all points in the plane.\\

\emph{Parellelogram rule for vector addition:} If \textbf{u} and \textbf{v} in $\R^2$ are represented as points in the plane, then $\textbf{u} + \textbf{v}$ corresponds to the fourth vertex of the parellolgram whose other vertices are $\textbf{u}$, $\textbf{0}$, and $\textbf{v}$.

\subsubsection{Algebraic Properties of $\R^n$}
For all $\bf{u}, \bf{v}, \bf{w}$ in $\R^n$ and all scalars $c$ and $d$:
\begin{enumerate}
	\item $\bf{u} + \bf{v} = \bf{v} + \bf{u}$
	\item $(\bf{u + v) + w = u + (v + w)}$
	\item $\bf{u + 0 = 0 + u = u}$
	\item $\bf{u + (-u) = -u + u = 0}$
	\item $c(\bf{u + v}$) = c$\bf{u}$ + c$\bf{v}$
	\item $(c + d)\bf{u} = c\bf{u} + d\bf{u}$
	\item $c(d\bf{u}) = (cd)\bf{u}$
	\item $1\bf{u = u}$
\end{enumerate}
\leavevmode \\
\emph{Linear combination:} a composite formed of vectors in $\R^n$ and corresponding scalar weights ($\bf{y}$$=c_1$$\bf{v_1+ ... + }$$c_p$$\bf{v_p}$)

\subsubsection{Example 5: Solve $x_1\bf{a_1}$$ + x_2 $$\bf{a_2} = \bf{b} \text{ for } \bf{a_1} = \begin{bmatrix}1\\-2\\-5\end{bmatrix}, \bf{a_2} = \begin{bmatrix}2\\5\\6\end{bmatrix}\text{, and  }\bf{b} = \begin{bmatrix}7\\4\\-3\end{bmatrix}.$}
We shall begin by constructing an augmented matrix and finding its reduced row echelon form:
\begin{center}
	$\begin{bmatrix}
		1 & 2 & 7\\
		-2 & 5 & 4\\
		-5 & 6 & -3
	\end{bmatrix}$
	\textasciitilde
	$\begin{bmatrix}
		1 & 2 & 7\\
		0 & 9 & 18\\
		0 & 16 & 32
	\end{bmatrix}$
	\textasciitilde
	$\begin{bmatrix}
		1 & 2 & 7\\
		0 & 1 & 2\\
		0 & 1 & 2
	\end{bmatrix}$
	\textasciitilde
	$\begin{bmatrix}
		1 & 2 & 7\\
		0 & 1 & 2\\
		0 & 0 & 0\\
	\end{bmatrix}$
\end{center}
So $x_2 = 2$ and $x_1 = 7 - 2x_2 = 3$.\\

Checking: $3\begin{bmatrix}1\\-2\\-5\end{bmatrix} + 2\begin{bmatrix}2\\5\\6\end{bmatrix} = \begin{bmatrix}3 + 4\\-6 + 10\\-15 + 12\end{bmatrix} = \begin{bmatrix}7\\4\\-3\end{bmatrix} \blacksquare.$\\





\emph{A vector $\bf{b}$ can be generated by a linear combination of $\bf{a_1, ..., a_n}$ iff there exists a solution to the linear system corresponding to the augmented matrix $\begin{bmatrix}\bf{a_1} & \bf{a_2} & ... & \bf{a_n} & \bf{b}\end{bmatrix}$.}\\
One of the key ideas in linear algebra is to study the set of all vectors that can be written as a linear combination of a fixed set $\{ \bf{v_1, ..., v_p} \}$ of vectors.\\

\emph{The subset of $\R^n$ spanned by $\bf{v_1}, ..., \bf{v_p}:$} (written Span$\{\bf{v_1, ..., v_p}\}$) is the collection of all vectors that can be written in the form $c_1$$\bf{v_1 }$$ + c_2$$\bf{v_2 }$$ + ... + c_p\bf{v_p}$ with $c_1, ..., c_p$ scalars.\\

Asking whether a vector is in Span$\{\bf{v_1, ..., v_p}\}$ amounts to asking whether the linear system with associated augmented matric has a solution.\\

Note: Span$\{\bf{v_1, ..., v_p}\}$ contains every scalar multiple of $\bf{v_1}$ including $\bf{0}$
\subsubsection{Geometric description of Span$\{\bf{v}\}$}
For $\bf{v} \in \R^3$, Span$\{\bf{v}\}$ is the set of points on the line in $\R^3$ through \textbf{v} and \textbf{0}.\\
For $\textbf{u, v} \in \R^3$, Span$\{\textbf{u, v}\}$ is the plane in $\R^3$ that contains \textbf{u}, \textbf{v}, and \textbf{0}. \\

\subsubsection{Example 6: Span$\{\mathbf{a_1, a_2}\}$ is a plane through the origin in 
$\R^3$. Is $\mathbf{b}$ in that plane?}
\begin{center}
	($\mathbf{a_1} = \begin{bmatrix}1\\-2\\3\end{bmatrix}, 
	\mathbf{a_2} = \begin{bmatrix}5\\-13\\-3\end{bmatrix}, 
	\text{and }\mathbf{b} = \begin{bmatrix}-3\\8\\1\end{bmatrix}$)
\end{center}

This question amounts to asking if the equation $x_1\mathbf{a_1} + x_2\mathbf{a_2} = \mathbf{b}$ has a solution. From row reduction:
\begin{center}
	$\begin{bmatrix}
		1 & 5 & -3\\
		-2 & -13 & 8\\
		3 & -3 & 1\\
	\end{bmatrix}
	\sim
	\begin{bmatrix}
		1 & 5 & -3\\
		0 & -3 & 2\\
		0 & -18 & 10\\
	\end{bmatrix}
	\sim
	\begin{bmatrix}
		1 & 5 & -3\\
		0 & -3 & 2\\
		0 & 0 & -2\\
	\end{bmatrix}$
\end{center}
This suggests that $0 = -2$ but as this is not possible, the matrix is inconsistent and so the vector equation has no solution. Thus, $\mathbf{b}$ is not in Span\{\textbf{$a_1, a_2$}\}. $\blacksquare$



%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 4: The Matrix Equation}
\begin{itemize}
\item $\in$: \indent belongs to \\
\item $\R^n$: \indent the set of vectors with \emph{n} real-valued elements\\
\item $\R^{m\times n}$: \indent the set of real-valued matrices with \emph{m} as rows and \emph{n} as columns\\
\end{itemize}
Example:
The notation $\vec{x} \in \R^5 m$ measn that $\vec{x}$ is a vector with five real-valued elements.\\

\subsubsection{Matrix-Vector Product as a Linear Combination}
If $A \in \R^{m\times n}$ has columns $\vec{a_1}, ..., \vec{a_n}$, then the matrix vector product $A\vec{x}$ is a linear combination of the columns of \emph{A}. 
$$A\vec{x} = \sum_{n=1}^n x_n \vec{a_n}$$
Note that $A\vec{x}$ is in the span of the columns of A.\\

This means that the solution sets for 
$$A\vec{x} = \vec{b}$$
is the same as
$$x_1\vec{a_1} + ... + x_n\vec{a_n} = \vec{b}$$
which is again equivalent to the set of linear equations with the augmented matrix
$$\begin{bmatrix}
	\vec{a_1} & \vec{a_2} & ... & \vec{a_n} & \vec{b_n}
\end{bmatrix}$$

\subsubsection{Example:} 
Suppose that $A = \begin{bmatrix}1 & 0\\ 0 & -3\end{bmatrix}$ and $\vec{x} = \binom{2}{3}$\\
\begin{enumerate}
\item The following product can be written as a linear combination of vectors:
$$A\vec{x} = 2 \binom{1}{0} + 3 \binom{0}{-3} = \binom{2}{-9}$$

\item Is $\vec{b} = \binom{2}{9}$ in the span of the columns of A?\\
If $\vec{b} \in Span\{A\}$, then $\vec{b} = c_1 \binom{1}{0} + c_2 \binom{0}{-3}$. This is true for $\vec{c} = \binom{2}{-3}$ so $\vec{b} \in Span\{A_{col}\}$
\end{enumerate}

\textbf{The equation \mateq  has a solution iff $\vec{b}$ is a linear combination of the columns of $A$}

\subsubsection{Example: For what vectors
$\vec{b} = \begin{bmatrix}b_1\\b_2\\b_3\\\end{bmatrix}$ does the equation have a solution?}
$$ \begin{bmatrix}
	1&3&4\\
	2&8&4\\
	0&1&-2\\
\end{bmatrix} \vec{x} = \vec{b}$$

Solution: 
$$\begin{bmatrix}
	1 & 3 & 4 & b_1\\
	2 & 8 & 4 & b_2\\
	0 & 1& -2 & b_3\\
\end{bmatrix} \sim 
\begin{bmatrix}
	1 & 3 & 4 & b_1\\
	0 & 2 & -4 & b_2 - 2b_1\\
	0 & 1& -2 & b_3\\
\end{bmatrix} \sim 
\begin{bmatrix}
	1 & 3 & 4 & b_1\\
	0 & 2 & -4 & b_2 - 2b_1\\
	0 & 0 & 0 & b_3 - \frac{1}{2}b_2 + b_1\\
\end{bmatrix}$$
So $$ \vec{b} = \begin{bmatrix} -\frac{1}{2}b_2 + b_3\\ b_2 \\ b_3\end{bmatrix}$$

Essential concept:
If $A$ is an $m\times n$ matrix, the following statements are logically equivalent -- for a particular $A$, \emph{all} are true or \emph{all} are false:
\begin{itemize}
	\item For each \textbf{b} in $\R^m$, the equation $A\mathbf{x} = \mathbf{b}$ has a solution.
	\item Each \textbf{b} in $\R^m$ is a linear combination of the columns of $A$
	\item The columns of $A$ span $\R^m$
	\item $A$ has a pivot position in every row
\end{itemize}

\subsubsection{Summary: Ways of representing Linear Systems}
\begin{enumerate}
	\item A list of equations
	\item An augmented matrix
	\item A vector equation
	\item A matrix equation
\end{enumerate}

\subsubsection{Matrix-vector products}
If $A$ is an $m\times n$ matrix, \textbf{u} and \textbf{b} are vectors in $\R^n$, and $c$ is a scalar, then:
\begin{itemize}
	\item $A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}$
	\item $A(c\mathbf{u}) = c(A\mathbf{u})$
\end{itemize}

Moreover, the product $A\mathbf{x}$ can be easily calculated by taking advantage of the nature of a Matrix-vector product:
$$\begin{bmatrix}
	a_1 & a_2 & a_3\\
	a_4 & a_5 & a_6\\
	a_7 & a_8 & a_9\\
\end{bmatrix}\begin{bmatrix}
	x_1\\
	x_2\\
	x_3\\
\end{bmatrix} = \begin{bmatrix}
	a_1 x_1 + a_2 x_2 + a_3 x_3\\
	a_4 x_1 + a_5 x_2 + a_6 x_3\\
	a_7 x_1 + a_8 x_2 + a_9 x_3\\
\end{bmatrix}$$

\emph{The identity matrix:} denoted $\mathbf{I}$, this $n\times n$ matrix contains 1's on the diagonal and 0's elsewhere, creating the universal property that $\mathbf{I_n x} = \mathbf{x} \text{  for every  } \mathbf{x} \in \R^n$

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 5: Solution sets of Linear Systems}
\emph{Homogenous:} characteristic of linear systems of the form \mateq, $\vec{b} = \vec{0}$\\

\emph{Inhomogeneous:} systems of the form \mateq, $\vec{b} \neq \vec{0}$\\

Because homogenous systems always have trivial solutions, the interesting question comes in asking whether they have non-trivial solutions.


\begin{center}
	$A \vec{x} = 0$ has a nontrivial solution $\iff$ there is a free variable $\iff A$ has a column with no pivot
\end{center}


\subsubsection{Example: Identify the free variables and the solution set for}
\begin{align*}
	x_1 + 3x_2 + x_3 &= 0\\
	2x_1 - x_2 - 5x_3 &= 0\\
	x_1 - 2x_3 &= 0
\end{align*}

$$\begin{bmatrix}
	1 & 3 & 1 & 0\\
	2 & -1 & -5 & 0\\
	1 & 0 & -2 & 0
\end{bmatrix} \sim
\begin{bmatrix}
	1 & 3 & 1 & 0\\
	0 & -7 & -7 & 0\\
	0 & -3 & -3 & 0
\end{bmatrix} \sim
\begin{bmatrix}
	1 & 3 & 1 & 0\\
	0 & 1 & 1 & 0\\
	0 & 1 & 1 & 0
\end{bmatrix} \sim
\begin{bmatrix}
	1 & 0 & -2 & 0\\
	0 & 1 & 1 & 0\\
	0 & 0 & 0 & 0\\
\end{bmatrix}$$

Row 3 has no pivot ($x_3$ is free) so there is a non-trivial solution. 
$$
\begin{cases}
	x_1 = &2x_3\\
	x_2 = &-x_3\\
	x_3 &\text{is free}
\end{cases} \implies x_3 \begin{bmatrix} 2 \\ -1\\ 1\end{bmatrix}$$
\pagebreak
\subsubsection{Parametric vector forms}
\emph{Parametric vector form:} a more convenient way of expressing the solutions of a linear system, taking advantage of the geometric interpretation of a linear system. In general, for free variables $x_k, ... x_n$ of $A\vec{x} = 0$, the solutions can all be written as 
$$\vec{x} = \sum_{n=k}^n x_n\vec{v_n}$$
In other words, solving an equation amounts to finding an explicit description of the solution plane as a set spanned by \textbf{u} and \textbf{v}. Thus, the equation from earlier describing the solution set can also be written as 
$$\mathbf{x} = s\mathbf{u} + t\mathbf{v} \quad (s, t \in \R)$$
emphasising the role of the free variables as arbitrary scalar multiples of the vectors forming a plane.

\subsubsection{Example: Describe all solutions of $A\mathbf{x} = \mathbf{b}$}
$$A = \begin{bmatrix}
	3 & 5 & -4\\
	-3 & -2 & 4\\
	6 & 1 & -8
\end{bmatrix} \text{and} \quad \mathbf{b} = \begin{bmatrix}
	7\\
	-1\\
	-4\\
\end{bmatrix}$$

$$\begin{bmatrix}
	3 & 5 & -4 & 7\\
	-3 & -2 & 4 & -1\\
	6 & 1 & -8 & -4
\end{bmatrix} \sim
\begin{bmatrix}
	1 & 0 & -\frac{4}{3} & -1\\
	0 & 1 & 0 & 2\\
	0 & 0 & 0 & 0\\
\end{bmatrix}$$

Thus $x_1 = -1 +\frac{4}{3}x_3,  x_2=2$, and $x_3$ is free. The general form vector is 
$$\mathbf{x} = \begin{bmatrix}
	-1\\
	2\\
	0\\
\end{bmatrix} + x_3\begin{bmatrix}
	\frac{4}{3}\\
	0\\
	1\\
\end{bmatrix} \implies \mathbf{x} = \mathbf{p} + x_3 \mathbf{v}$$
By replacing $x_3$ with a general parameter $t \in \R$, we have a universal solution for nonhomogenous systems:

\textbf{$\mathbf{x} = \mathbf{p} + t \mathbf{v}$ is a solution for $Ax = b$ which is parallel to the line of the solution set $Ax = 0$ because it is a translation of v by the particular solution p} 

\subsubsection{Theorem:}
Suppose the equation $Ax=b$ is consistent for some given \textbf{b} and let \textbf{p} be a solution. The solution set of $Ax =b$ is the set of all vectors of the form $\mathbf{w} = \mathbf{p} + \mathbf{v}_h$ where $\mathbf{v}_h$ is any solution of the homogenous equation $Ax=b$
%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 6: Linear Independence}
A set of vectors {$\vec{v_1}, ..., \vec{v_k}$} $\in \R^n$ are linearly independent if $\sum_{n=1}^k = c_k \vec{v_k} = 0$ has only the trivial solution ($\vec{c} = \vec{0}$). It is linearly dependent otherwise. 

Establishing linear independence is thus equivalent to asking whether the equation $V\vec{c} = 0$ ($V \in \R^k :=$ the matrix corresponding to the linear combinations of the vectors) is only true for $\vec{c} = \vec{0}$

Example: For what values of $h$ is the set of vectors linearly independent? 
$$
	\begin{bmatrix}
		1\\1\\h
	\end{bmatrix}, 
	\begin{bmatrix}
		1\\h\\1
	\end{bmatrix}, 
	\begin{bmatrix}
		h\\1\\1
	\end{bmatrix}
$$
$$
\begin{bmatrix}
	1 & 1 & h & 0\\
	1 & h & 1 & 0\\
	h & 1 & 1 & 0\\
\end{bmatrix} \sim 
\begin{bmatrix}
	1 & 1 & h & 0\\
	0 & h - 1 & 1 - h & 0\\
	0 & 1 - h & 1 - h^2 & 0\\
\end{bmatrix} \sim 
\begin{bmatrix}
	1 & 1 & h & 0\\
	0 & h - 1 & 1 - h & 0\\
	0 & 0 & 2 - h - h^2 & 0\\
\end{bmatrix}
$$

If $2 - h - h^2 = 0$ then we have a free variable and the vectors will be linearly dependent (because there will be more solutions than the trivial case).

Factoring, we get 
$$0 = -(h + 2)(h - 1)$$
so for the vectors to be independent, 
$$h \neq \{-2, 1\}$$

\subsubsection{Linear Independence Theorems}
\begin{enumerate}
	\item More Vectors Than Elements
		\indent For vectors $\vec{v}_1, ..., \vec{v}_k \in \R^n$, if $k > n$, then $\{\vec{v}_1, ..., \vec{v}_k\}$ is linearly dependent (because not every column of the matrix $A = (\vec{v}_1, ..., \vec{v}_k)$ would be pivotal). 
	\item Set Contains Zero Vector
		\indent If any one or more of $\vec{v}_1, ..., \vec{v}_k$ is $\vec{0}$, then $\{\vec{v}_1, ..., \vec{v}_k\}$ is linearly dependent (again because there would be non-pivotal columns of the corresponding matrix).
	\item A set containing only one vector is linearly independent iff \textbf{v} is not the zero vector
	\item Sets of two vectors can be determined to be linearly dependent by inspection if one is a multiple of the other.
	\item In geometric terms, two vectors are linearly dependent iff they lie on the same lie through the origin.
\end{enumerate}

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 7: Introduction to Linear Transformations}
Instead of as a linear combination of variables, matrices can also be viewed as a \emph{function} from one set of vectors to another. Thus, solving the equation $Ax=b$ amounts to finding all vectors \textbf{x} in $\R^4$ that are transformed into the vector \textbf{b} in $\R^2$ under the "action" of multiplication by $A$.\\ 

\emph{Transformation (function, mapping):} a rule that assigns to each vector $\mathbf{x} \in \R^n$ a vector $T(x) \in \R^m$. \\

\emph{Domain:} The set $\R^n$ is the domain of T ($T \in \R^n$)

\emph{Codomain:} The set $\R^m$ is the codomain of T ($T \in \R^n$)\\

This domain-codomain relationship is expressed in the notation $T: \R^n \to \R^m$\\

\emph{Image:} this is the "resultant" vector of the transformation 
$$T(x)\in \R^m \text{ for } \mathbf{x}\in \R^n \implies$$
\begin{center}
 "The vector T(x) is the image of x under the action of T"
\end{center}

\emph{Range:} The set of all images of T(x) is the range of T. This is equivalent to the Span of the columns of a matrix.

For example, $f(x) = \sin x : \R \mapsto \R$, so the domain is $\R$, the codomain is $\R$, and the range is [-1, 1]

A function $T : \R^n \mapsto \R^m$ is linear if
\begin{itemize}
	\item $T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v}) \quad \forall \vec{u}, \vec{v} \in \R^n$
	\item $T(c\vec{v}) = cT(\vec{v}) \quad \forall \vec{v} \in \R^n, c \in \R$
\end{itemize}

\emph{Principle of superposition:}
$$T(c_1 \vec{v}_1 + ... + c_k \vec{v}_k) = c_1 T(\vec{v}_1 + ... + c_k T(\vec{v}_k)$$

\textbf{Every matrix transformation $T_A$ is linear.}

\subsubsection{Matrix Transformations}
A matrix transformation (a mapping where $\forall \mathbf{x} \in \R^n, T(x) =  A\mathbf{x} \quad (A \in \R^{m \times n})$) can be written as $\mathbf{x} \mapsto A\mathbf{x}$\\
Similarly, every matrix transform is completely determined by what it does to the columns of the $n\times n$ identity matrix.

Geometric interpretations of transforms in $\R^2$:
\begin{enumerate}
	\item $A = \begin{bmatrix}
		0 & 1\\1 & 0\\	
		\end{bmatrix}$ is a reflection through $x_1 = x_2$
	
	\item $A = \begin{bmatrix}
		1 & 0\\0 & 0\\	
	\end{bmatrix}$ is a projection onto the x-axis ($\begin{bmatrix}
		x_1\\ 0\\
	\end{bmatrix}$)

	\item $A = \begin{bmatrix}
		k & 0\\0 & k\\	
	\end{bmatrix}$ is a scaling by k 
\end{enumerate}

Geometric interpretations of transforms in $\R^2$:
\begin{enumerate}
	\item $A = \begin{bmatrix}
		1&0&0\\
		0&1&0\\
		0&0&0\\
		\end{bmatrix}$ is a projection onto the $x_1, x_2$-plane ($T(\vec{x}) = \begin{bmatrix}
		a\\b\\0
		\end{bmatrix}$)
	\item $A = \begin{bmatrix}
			1&0&0\\
			0&-1&0\\
			0&0&1\\
		\end{bmatrix}$ is a reflection through the $x_1, x_3$-plane ($T(\vec{x}) = \begin{bmatrix}
		a\\-b\\c
		\end{bmatrix}$)
\end{enumerate}

\emph{Standard vectors in $\R^n$:}
$$\vec{e}_1 = \begin{bmatrix}
	1\\0\\0\\\vdots\\0\
\end{bmatrix}, \quad \vec{e}_2 = \begin{bmatrix}
	0\\1\\0\\\vdots\\0
\end{bmatrix}, \quad \vec{e}_n = \begin{bmatrix}
	0\\0\\0\\\vdots\\1
\end{bmatrix}$$

Multiplying a matrix by $\vec{e}_i$ gives column $i$ of $A$

\subsubsection{Standard transform matrices}
Counterclockwise rotation by angle $\theta$ about (0, 0):
$$T(x) = A\vec{x} = \begin{bmatrix}
	\cos \theta & -\sin \theta\\
	\sin \theta & \cos \theta\\
\end{bmatrix}$$

Reflection through $x_1$-axis:
$$T(x) = \begin{bmatrix}
	1 & 0\\
	0 & -1\\
\end{bmatrix}$$

Reflection through $x_2$-axis:
$$T(x) = \begin{bmatrix}
	-1 & 0\\
	0 & 1\\
\end{bmatrix}$$

Reflection through $x_2 = x_1$:
$$T(x) = \begin{bmatrix}
	0 & 1\\
	1 & 0\\
\end{bmatrix}$$

Reflection through $x_2 = -x_1$
$$T(x) = \begin{bmatrix}
	0 & -1\\
	-1 & 0\\
\end{bmatrix}$$

Horizontal contraction:
$$T(x) = \begin{bmatrix}
	k & 0\\
	0 & 1\\
\end{bmatrix}, \quad |k| < 1$$

Horizontal expansion:
$$T(x) = \begin{bmatrix}
	k & 0\\
	0 & 1\\
\end{bmatrix}, \quad |k| > 1$$

Vertical contraction:
$$T(x) = \begin{bmatrix}
	1 & 0\\
	0 & k\\
\end{bmatrix}, \quad |k| < 1$$

Vertical expansion:
$$T(x) = \begin{bmatrix}
	1 & 0\\
	0 & k\\
\end{bmatrix}, \quad |k| > 1$$

Horizontal shear (left):
$$T(x) = \begin{bmatrix}
	1 & k\\
	0 & 1\\
\end{bmatrix}, \quad k < 0$$

Horizontal shear (right):
$$T(x) = \begin{bmatrix}
	1 & k\\
	0 & 1\\
\end{bmatrix}, \quad k > 0$$

Vertical shear (down):
$$T(x) = \begin{bmatrix}
	1 & 0\\
	k & 1\\
\end{bmatrix}, \quad k < 0$$

Vertical shear (up):
$$T(x) = \begin{bmatrix}
	1 & 0\\
	k & 1\\
\end{bmatrix}, k > 0$$

Projection onto the $x_1$-axis:
$$T(x) = \begin{bmatrix}
	1 & 0\\
	0 & 0\\
\end{bmatrix}$$

Projection onto the $x_2$-axis:
$$T(x) = \begin{bmatrix}
	0 & 0\\
	0 & 1\\
\end{bmatrix}$$

 \emph{Onto transform:} a transform for which $A\vec{x} = \vec{b}$\\
 \quad - This is an existence property, for any $\vec{b} \in \R^m, A\vec{x} = \vec{b}$ has a solution\\
 \quad - T is onto iff its standard matrix has a pivot in every row

 \emph{One-to-one transform:} a transform for which there is at most one solution to $ A\vec{x} = \vec{b}$\\
 \quad - This uniqueness property does not assert existence for all $\vec{b}$\\
 \quad - T is one-to-one iff the only solution to $T(\vec{x}) = \vec{0}$ is $\vec{x} = \vec{0}$\\
 \quad - T is one-to-one iff every column of A is pivotal\\

 Onto means that the columns of A span $\R^n$

 One-to-one means that the only solution is the trivial one and A has linearly independent columns

\subsubsection{Determining the image of a transform}
Example: Suppose T is a transform $T: \R^2 \mapsto \R^3$ such that 
$$T(\vec{e}_1) = \begin{bmatrix}
	5\\
	-7\\
	2
\end{bmatrix} \quad T(\vec{e}_2) = \begin{bmatrix}
	-3\\
	8\\
	0\\
\end{bmatrix}$$

As any matrix can be written as a linear combination
$$\vec{x} = \begin{bmatrix}
	x_1 \\ x_2
\end{bmatrix} = x_1 \begin{bmatrix}
	1 \\ 0
\end{bmatrix} + x_2 \begin{bmatrix}
	0 \\ 1
\end{bmatrix} = x_1 \vec{e}_1 + x_2 \vec{e}_2$$

And since T is a linear transformation
\begin{align*}
	T(\vec{x}) &= x_1 T(\vec{e}_1) + x_2 T(\vec{e}_2)\\
	&= x_1 \begin{bmatrix}
		5\\-7\\2
	\end{bmatrix} + x_2\begin{bmatrix}
		-3\\8\\0
	\end{bmatrix} = \begin{bmatrix}
		5x_1 - 3x_2\\
		-7x_1 + 8x_2\\
		2x_1 + 0\\
	\end{bmatrix}
\end{align*}

\subsubsection{Example: Constructing a standard matrix}
Define a linear transformation by
$$T(x_1, x_2) = (3x_1 + x_2, 5x_1 + 7x_2, x_1 + 3x_2)$$

$$A = \begin{bmatrix}
	T(\vec{e}_1) & T(\vec{e}_2)
\end{bmatrix}$$
\begin{align*}
	T(e_1) &= T(1, 0) = \begin{bmatrix}
		3\\5\\1
	\end{bmatrix}\\
	T(e_2) &= T(0, 1) = \begin{bmatrix}
		1\\7\\3
	\end{bmatrix}
\end{align*} 
$$A = \begin{bmatrix}
		3 & 1\\
		5 & 7\\
		1 & 3\\
	\end{bmatrix}$$

Because there are more columns than rows, it cannot be onto.
Because the two columns are not multiples of each other by inspection, the columns are linearly independent so the matrix is one-to-one.


%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\section{Module 2: Matrix Algebra}
\subsection{TOPIC 1: Matrix Operations}
\emph{Main diagonal:} the diagonal entries $a_{11}, a_{22}, a_{33}, ...$ of $A=[a_{ij}], \quad A\in \R^{m\times n}$

\emph{Diagonal matrix:} a square matrix $n \times n$ whose nondiagonal entries are zero (e.g. $I_n$)

\emph{Zero matrix:} a matrix whose entries are all zero, written $0$

\emph{Equal:} property of two matrices of the same size with corresponding entries equal

Properties of Matrix addition:
\begin{enumerate}
	\item $A + B = B + A$
	\item $(A + B) + C = A + (B + C)$
	\item $A + 0 = A$
	\item $r(A + B) = rA + rB$
	\item $(r + s)A = rA + sB$
	\item $r(sA) = (rs)A$
\end{enumerate}

If a matrix $B$ multiplies a vector $\mathbf{x}$, it is a transform $B\mathbf{x}$. If this vector is then multiplied by $A$, the composite transformed vector is $A(Bx)$. Thus:
$$A(Bx) = (AB)x$$

$$AB = A\begin{bmatrix}
	\mathbf{b}_1 & \mathbf{b}_2 & ... & \mathbf{b}_p
\end{bmatrix} = \begin{bmatrix}
	A\mathbf{b}_1 & A\mathbf{b}_2 & ... &A\mathbf{b}_p
\end{bmatrix}$$

Multiplication of matrices corresponds to composition of linear transformations.

Row column rule for matrix multiplication:
If $A \in \R^{m \times n}$ has rows $\vec{a}_i$, and $B \in \R^{n\times p}$ has columns $\vec{b}_j$, each element of the product $C = AB$ is the dot product $c_{ij} = \vec{a}_i \cdot \vec{b}_j$

Each column of AB is a linear combination of the columns of A using weights from the corresponding column of B. 

\textbf{The number of columns of A must match the number of rows in B in order for a linear combination such as $A\mathbf{b}_1$ to be defined.}

Properties of Matrix multiplication:
\begin{enumerate}
	\item $A(BC) = (AB)C$
	\item $A(B + C) = AB + AC$
	\item $(B + C)A = BA + CA$
	\item $r(AB) = (rA)B = A(rB)$
	\item $I_mA = A = AI_n$
	\item If $AB = AC$ it is usually NOT true that $B = C$
	\item If $AB$ is the zero matrix, you CANNOT conclude that either $A = $  or $B = 0$
\end{enumerate}

If $A \neq 0$ and $\mathbf{x} \in \R^n$, then $A^k\mathbf{x}$ is the result of left-multiplying \textbf{x} by A repeatedly k times. ($A^0 = I_n$)

\emph{Transpose:} the transpose of an $m \times n$ matrix A is the $n \times m$ matrix $A^T$ whose columns are the corresponding rows of A.
$$A = \begin{bmatrix}
	a & b\\
	c & d
\end{bmatrix} \quad A^T = \begin{bmatrix}
	a & c\\
	b & d
\end{bmatrix}$$

Transposition properties:
\begin{enumerate}
	\item $(A^T)^T = A$
	\item $(A + B)^T = A^T + B^T$
	\item $(rA)^T = rA^T$
	\item $(AB)^T = B^TA^T$
\end{enumerate}

Hence, \textit{the transpose of a product of matrices equals the product of their transposes in the reverse order.}

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 2: The Inverse of a Matrix}
\emph{Invertible:} a square matric is invertible if there is a matrix $C \in \R^{n \times n}$ such that 
$$CA = I \quad \text{and} \quad AC = I$$
Here, C is the unique inverse of A often denoted $A^-1$

\emph{Singular matrix:} a noninvertible matrix 

\emph{Nonsingular matrix:} an invertible matrix

\subsubsection{Finding the inverse}
$$A = \begin{bmatrix}
	a & b\\
	c & d\\
\end{bmatrix}$$
If $ad - bc \neq 0$, then A is invertible and 
$$A^{-1} = \frac{1}{ad - bc}\begin{bmatrix}
	d & -b\\
	-c & a
\end{bmatrix}$$

\emph{Determinant:} $\det A = ad - bc$. If $\det A = 0$, A is not invertible. 

\textbf{If A is an invertible $n \times n$ matrix, then for each $\vec{b}$ in $\R^n$, the equation $Ax = \vec{b}$ has the unique solution $\vec{x} = A^{-1}\vec{b}$}

While the equation above can used to solve an equation $Ax = b$, row reduction of $\begin{bmatrix}
	A & \mathbf{b}
\end{bmatrix}$ is almost always faster.

Properties of invertible matrices:
\begin{enumerate}
	\item $(A^-1)-1 = A$
	\item $(AB)^-1 = B^-1A^-1$
	\item $(A^T)^-1 = (A^-1)^T$
\end{enumerate}

\emph{Elementary matrix:} a matric that is obtained by performing a single elementary row operation on an identity matrix.

If an elementary row operation is performed on an $m \times n$ matrix A, the resulting matrix can be written as $EA$, where the $m \times m$ matrix $E$ is created by performing the same row operation on $I_m$.

\textbf{Every elementary matrix is invertible and square} where the inverse of E is the elementary matrix that trasforms E back into I.

\emph{An $n \times n$ matrix is invertible iff A is row equivalent to $I_n$, in which case any sequence of elemenary row operations which maps $A \mapsto I_n$ also transforms $I_n \mapsto A^{-1}$}

$A$ has an inverse iff for all $\vec{b} \in \R^n$, $Ax=b$ has a unique solution

\subsubsection{To find $A^{-1}$:}
Row reduce the augmented matrix $\begin{bmatrix}
	A & I
\end{bmatrix}$ If A is row equivalent to I, then $\begin{bmatrix}
	A & I
\end{bmatrix}$ is row equivalent to $\begin{bmatrix}
	I & A^{-1}
\end{bmatrix}$

Example: 
Find the inverse of $A = \begin{bmatrix}
	0 & 1 & 2\\
	1 & 0 & 3\\
	4 & -3 & 8
\end{bmatrix}$

Solution:
$$\begin{bmatrix}
	A & I
\end{bmatrix} = \begin{bmatrix}
	0 & 1 & 2 & 1 & 0 & 0\\
	1 & 0 & 3 & 0 & 1 & 0\\
	4 & -3 & 8 & 0 & 0 & 1
\end{bmatrix} \sim \begin{bmatrix}
	1 & 0 & 0 & -\frac{9}{2} & 7 & -\frac{3}{2}\\
	0 & 1 & 0 & -2 & 4 & -1\\
	0 & 0 & 1 & \frac{3}{2} & -2 & \frac{1}{2}
\end{bmatrix}$$
Since $A \sim I$, A is invertible and 
$$A^{-1} = \begin{bmatrix}
	-\frac{9}{2} & 7 & -\frac{3}{2}\\
	-2 & 4 & -1\\
	\frac{3}{2} & -2 & \frac{1}{2}
\end{bmatrix}$$

Row reduction of $\begin{bmatrix}
	A & I
\end{bmatrix}$ to $\begin{bmatrix}
	I & A^{-1}
\end{bmatrix}$ can be viewed as the simultaneous solution of the n systems 
$$A\mathbf{x} = \mathbf{e_1}, \quad A\mathbf{x} = \mathbf{e_2}, \quad ..., \quad A\mathbf{x} = \mathbf{e_n}$$

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 3: Characterisations of Invertible Matrices}

\emph{Invertible matrix theorem:} for a given square matrix A, the following are all true or all false:
\begin{enumerate}
	\item A is an invertible matrix
	\item A is row equivalent to $I_n$
	\item A has n pivot positions
	\item The equation $Ax = 0$ has only the trivial solution
	\item THe columns of A form a linearly independent set
	\item The linear transformation $x \mapsto Ax$ is one-to-one
	\item The equation $Ax = b$ has at least one solution for each $\mathbf{b} \in \R^n$
	\item The columns of A span $\R^n$
	\item The linear transformation $x \mapsto Ax$ maps $\R^n$ onto $\R^n$
	\item There is an $n \times n$ matrix C such that $CA = I$
	\item There is an $n \times n$ matrix D such that $AD = I$ 
	\item $A^T$ is an invertible matrix
\end{enumerate}

If $A$ and $B$ are two square matrices and $AB = I$, then both $A$ and $B$ are invertible with $B = A^{-1}$ and $B^{-1}$

\emph{Invertible linear transformation}: a transform is invertible if there exists a function $S: \R^n \to \R^n$ such that 
\begin{align}
	S(T(x)) &= x \quad \forall x \in \R^n\\
	T(S(x)) &= x \quad \forall x \in \R^n\\
\end{align}
If such S exists, it is unique and must be a linear transform. This transform is invertible iff its standard matrix A is nonsingular. In that case, $S(x) = A^{-1}x$ is the unique function satisfying (1) and (2).

\emph{Ill-conditioned matrix:} an invertible matrix that can become singular if some of its entries are changed just slightly. This becomes dangerous if roundoff error makes a nonsingular matrix appear invertible.

%----------------------------------------------------------------------------------------------------------------------------------------%
\pagebreak
\subsection{TOPIC 4: Partitioned Matrices}
\emph{Partitioned matrix:} a matrix whose entries are meant to be considered differently than a list of column vectors based on the system it represents. 
For example, the matrix 
$$A = \left[ \begin{array}{ccc|cc|c}
	3 & 0 & -1  & 5 & 9 & -2\\
	-5 & 2 & 4 & 0 & -2 & 1\\
	\hline
	-8 & -6 & 3 & 1 & 7 & -4\\	
\end{array} \right]$$
can also be the $2 \times 3$ partitioned matrix 
$$A = \begin{bmatrix}
	A_{11} & A_{12} & A_{13}\\
	A_{21} & A_{22} & A_{23}
\end{bmatrix}$$
whose entries are the blocks (submatrices)
\begin{align*}
	A_{11} &= \begin{bmatrix}
		3 & 0 & -1\\
		-5 & 2 & 4
	\end{bmatrix}, & A_{12} &= \begin{bmatrix}
		5 & 9\\
		0 & -3
	\end{bmatrix}, & A_{13} &= \begin{bmatrix}
		-2\\
		1
	\end{bmatrix}\\
	A_{21} &= \begin{bmatrix}
		-8 & -6 & 3
	\end{bmatrix}, & A_{22} &= \begin{bmatrix}
		1 & 7
	\end{bmatrix}, & A_{23} &= \begin{bmatrix}
		-4
	\end{bmatrix}
\end{align*}

Assuming the blocks of two partitioned matrices are the same size, addition between them is defined as the block-by-block sum of corresponding partitions. Similarly, multiplication by a scalar is performed piecewise. 

Partitioned matrices can be multiplied by the usual row-column rule as if the block entires were scalars, provided that for a product AB, the column partition of A matches the row partition of B. 

\emph{Conformable:} property of two matrices that are partitioned the same way

\emph{Block diagonal matrix:} a paritioned matrix with zero blocks off the main diagonals 
\quad - such a matrix is invertible iff each block on the diagonal is invertible

Paritioned matrices are especially useful in numerical slutions of especially large systems where partitioning can make more efficient use of system resources

%----------------------------------------------------------------------------------------------------------------------------------------%

\subsection{TOPIC 5: MATRIX FACTORISATIONS}
\emph{Factorization:} an equation that expresses a matrix A as a product of two or more matrices. 

Whereas multiplication was a synthesis of data, factorisation is an analysis.

\subsubsection{LU Factorisation}
This is a more efficient process for solving the system
$$Ax = b_1, \quad Ax= b_2, \quad ..., \quad Ax = b_p$$

The method:
\begin{enumerate}
	\item Assume A is an $m \times n$ matrix that can be row reduced to echelon form without row interchanges
	\item A can be written in the form $A = LU$ where L is an $m \times n$ lower triangular matrix with 1's on the diagonal and U is an $m \times n$ echelon form of A
	$$A = LU = \begin{bmatrix}
		1 & 0 & 0 & 0\\
		* & 1 & 0 & 0\\
		* & * & 1 & 0\\
		* & * & * & 1\\
	\end{bmatrix} \begin{bmatrix}
		\blacksquare & * & * & * & *\\
		0 & \blacksquare & * & * & *\\
		0 & 0 & 0 & \blacksquare & *\\
		0 & 0 & 0 & 0 & 0\\
	\end{bmatrix}$$
	\item When $A = LU, Ax = b$ can be written as $L(Ux) = b$
	\item We can then find x by solving the pair of equations
		\begin{align*}
			Ly &= b\\
			Ux &= y
		\end{align*}
	\item First solve for y, then for x. Because L and U are triangular, both equations are easy to solve. 
\end{enumerate}

Example:
Given 
$$A = \begin{bmatrix}
	3 & -7 & -2 & 2\\
	-3 & 5 & 1 & 0\\
	6 & -4 & - & -5\\
	-9 & 5 & -5 & 12
\end{bmatrix} = \begin{bmatrix}
	1 & 0 & 0 & 0\\
	-1 & 1 & 0 & 0\\
	2 & -5 & 1 & 1\\
	-3 & 8 & 3 & 1
\end{bmatrix} \begin{bmatrix}
	3 & -7 & -2 & 2\\
	0 & -2 & -1 & 2\\
	0 & 0 & -1 & -1\\
	0 & 0 & 0 & -1
\end{bmatrix} = LU$$
use this LU factorisation to solve $Ax = b$, where $b = \begin{bmatrix}
	-9\\
	5\\
	7\\
	11
\end{bmatrix}$

Solution:
$$\begin{bmatrix}
	L & b
\end{bmatrix} = \begin{bmatrix}
	1 & 0 & 0 & 0 & -9\\
	-1 & 1 & 0 & 0 & 5\\
	2 & -5 & 1 & 1 & 7\\
	-3 & 8 & 3 & 1 & 11
\end{bmatrix} \sim \begin{bmatrix}
	1 & 0 & 0 & 0 & -9\\
	0 & 1 & 0 & 0 & -4\\
	0 & 0 & 1 & 0 & 5\\
	0 & 0 & 0 & 1 & 1
\end{bmatrix} = \begin{bmatrix}
	I & y
\end{bmatrix}$$
Then, 
$$\begin{bmatrix}
	U & y
\end{bmatrix} = \begin{bmatrix}
	3 & -7 & -2 & 2 & -9\\
	0 & -2 & -1 & 2 & -4\\
	0 & 0 & -1 & -1 & 5\\
	0 & 0 & 0 & -1 & 1
\end{bmatrix} \sim \begin{bmatrix}
	1 & 0 & 0 & 0 & 3\\
	0 & 1 & 0 & 0 & 4\\
	0 & 0 & 1 & 0 & -6\\
	0 & 0 & 0 & 1 & -1
\end{bmatrix}$$
In this method, finding x requires 28 arithmetic operations. Meanwhile, row reduction of $\left[A \quad b\right]$ to  $\left[I \quad x\right]$ takes 62 operations.

\subsubsection{An LU factorisation algorithm}
Suppose A can be reduced to an echelon form U using only row replacements that add a multiple of one row to another row \emph{below it}. In this case, there exist unit lower triangular elementary matrices such that 
$$E_p ... E_1A = U$$
Then 
$$A = (E_p ... E_1)^{-1}U = LU$$
where 
$$L = (E_p ... E_1)^{-1}$$
It can be shown that the row operations which reduce A to U also reduce L to I, because $E_p...E_1L=(E_p...E_1)(E_p...E_1)^{-1} = I$

In summary:
\begin{enumerate}
	\item Reduce A to an echelon form U by a sequence of row replacement operations, if possible.
	\item Place entries in L such that the same sequence of row operations reduces L to I. 
\end{enumerate}

Example:
Find an LU factorization of 
$$A = \begin{bmatrix}
	2 & 4 & -1 & 5 & -2\\
	-4 & -5 & 3 & -8 & 1\\
	2 & -5 & -4 & 1 & 8\\
	-6 & 0 & 7 & -3 & 1
\end{bmatrix}$$


Solution: 
Since A has four rows, L should be $4 \times 4$. The first column of L is the first column of A divided by the top pivot entry
$$L = \begin{bmatrix}
	1 & 0 & 0 & 0\\
	-2 & 1 & 0 & 0\\
	1 & & 1 & 0\\
	-3 & & & 1\\
\end{bmatrix}$$

Next row reduce A to its echelon form U:
\begin{align*}
	A &= \begin{bmatrix}
		2 & 4 & -1 & 5 & -2\\
		-4 & -5 & 3 & -8 & 1\\
		2 & -5 & -4 & 1 & 8\\
		-6 & 0 & 7 & -3 & 1
	\end{bmatrix} \sim \begin{bmatrix}
		2 & 4 & -1 & 5 & -2\\
		0 & 3 & 1 & 2 & -3\\
		0 & -9 & -3 & -4 & 10\\
		0 & 12 & 4 & 12 & -5
	\end{bmatrix} \sim \begin{bmatrix}
		2 & 4 & -1 & 5 & -2\\
		0 & 3 & 1 & 2 & -3\\
		0 & 0 & 0 & 2 & 1\\
		0 & 0 & 0 & 4 & 7
	\end{bmatrix}\\ 
	&\sim \begin{bmatrix}
		2 & 4 & -1 & 5 & -2\\
		0 & 3 & 1 & 2 & -3\\
		0 & 0 & 0 & 2 & 1\\
		0 & 0 & 0 & 4 & 7
	\end{bmatrix} \sim \begin{bmatrix}
		2 & 4 & -1 & 5 & -2\\
		0 & 3 & 1 & 2 & -3\\
		0 & 0 & 0 & 2 & 1\\
		0 & 0 & 0 & 0 & 5
	\end{bmatrix} = U
\end{align*}

The columns eliminated in each step of the row reduction process divided by the pivot form L:
$$\begin{bmatrix}
	2 & & & \\
	-4 & 3 & & \\
	2 & -9 & 2 & \\
	-6 & 12 & 4 & 5
\end{bmatrix} \implies L = \begin{bmatrix}
	1 & 0 & 0 & 0\\
	-2 & 1 & 0 & 0\\
	1 & -3 & 1 & 0\\
	-3 & 4 & 2 & 1\\
\end{bmatrix}$$

%----------------------------------------------------------------------------------------------------------------------------------------%

\subsection{TOPIC 6: The Leontief Input-Output Model}
The interactions between sectors can be represented by the matrix equation
$$(I - C)x = d$$
Where:
$I$ is the identity matrix
$C$ is a coefficient matrix representing the demand of each sector for goods from each other sector
$\mathbf{x}$ is a unit consumption vector describing the number of units  of output which are routed to each sector based on the consumption matrix 
$\mathbf{d}$ is the final demand vector (the output of the system after accounting for intermediate demand, $Cx$)

If $C$ and $\mathbf{d}$ have nonnegative entries and if each column sum of $C$ is less than 1, 
$$\mathbf{x} = (I - C)^{-1} \mathbf{d}$$
and
$$(I - C)^{-1} \approxeq I + C + C^2 + ... C^m$$
which, in most real-world circumstances, will have powers of the consumption matrix approach 0 quite quickly.

%----------------------------------------------------------------------------------------------------------------------------------------%
\subsection{TOPIC 7: Computer Graphics}

Often computer graphics will be represented by a list of points which can be connected by straight lines. 

\subsubsection{2D Graphics}
A particular graphic, for example the letter "N" can be stored as coordinates in a data Matrix
$$D = \begin{bmatrix}
	x\\y
\end{bmatrix} = \begin{bmatrix}
	0 & 0.5 & 0.5 & 6 & 6 & 5.5 & 5.5 & 0\\
	0 & 0 & 6.42 & 0 & 8 & 8 & 1.58 & 8
\end{bmatrix}$$

This represetation makes the transformation or animation of pictures very easy: the resultant figure is described by the matrix product $AD$ where A is any linear transformation.

Unfortunately, translation is not a linear transform so cannot be represented by matrix multiplication. 

\emph{Homogeneous coordinates:} corresponding sets of coordinates where $\forall x, y \in \R^2, (x, y) \mapsto (x, y, 1) \in \R^3$
For example, (0, 0) has homogenous coordinates (0, 0, 1). These coordinates are not multiplied or added by scalars but can be transformed via multiplication by $3 \times 3$ matrices. 

With homogeneous coordinates we are able to represent translations:
$$(x, y) \mapsto (x + h, y + k) \implies (x, y, 1) \mapsto (x + h, y + k, 1)$$
Thus, 
$$\begin{bmatrix}
	1 & 0 & h\\
	0 & 1 & k\\
	0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
	x\\
	y\\
	1
\end{bmatrix} = \begin{bmatrix}
	x + h\\
	y + k\\
	1
\end{bmatrix}$$

More broadly, any linear transformation in $\R^2$ can be represented in homogeneous coordinates by a paritioned matrix of the form $\begin{bmatrix}
	A & 0\\
	0 & 1
\end{bmatrix}$ where $A$ is a $2 \times 2$ standard matrix.

\subsubsection{3D Graphics}
Homogeneous coordinates for the point (x, y, z) can usually be expressed as (X, Y, Z, H) where 
$$x = \frac{X}{H}, \quad y = \frac{Y}{H}, \quad z = \frac{Z}{H}$$

%----------------------------------------------------------------------------------------------------------------------------------------%
\subsection{TOPIC 8: Subspaces of $\R^n$}
\emph{Subspace:} any set H in $\R^n$ that has the properties:
\begin{itemize}
	\item The zero vector is in H
	\item $\forall \mathbf{u}, \mathbf{v} \in H \quad \mathbf{u + v} \in H$
	\item $\forall \mathbf{u} \in H, \quad c\mathbf{u} \in H$
\end{itemize}

A subspace is closed under addition and scalar multiplication. 

One of the most common ways of visualising a subspace is a plane through the origin. 

Example: If $\mathbf{v}_1, \mathbf{v}_2 \in \R^n$ and $H = \text{Span}\{\mathbf{v}_1, \mathbf{v}_2\}$, then H is a subspace of $\R^n$

\emph{Column space:} the subspace of a matrix A is the set Col A of all linear combinations of the columns of A

Col A equals $\R^m$ only when the columns of A span $\R^m$. Otherwise, Col A is only part of $\R^m$

A vector b is in Col A iff the equation Ax = b has a solution. 

\emph{Null space:} the null space of a matrix A is the set Nul A of all solutions of the homogeneous equation Ax = 0

The null space of an $m \times n$ matrix A is a subspace of $\R^n$. Equivalently, the set of all solutions of a system $Ax = 0$ of m homogeneous linear equations in n unknowns is a subspace of $\R^n$. 

\emph{Basis:} for a subspace H of $\R^n$, the basis is a linearly independent set in H that spans H

\emph{Standard basis:} the set $\{\mathbf{e}_1, ..., \mathbf{e}_2\}$

The pivot columns of a matrix A form a basis for the column space of A.
Note: the columns of an echelon form B are often not in the column space of A

%----------------------------------------------------------------------------------------------------------------------------------------%

\subsection{TOPIC 9: Dimension and Rank}
The main reason for a selecting a basis for a subspace H rather than just a spanning set is that each vector in H can bbe written in only one way as a linear combination of the basis vectors.

\emph{Coordinate vector:} for each x in a subspace H, 
$$[x]_\mathfrak{B} = \begin{bmatrix}
	c_1\\ \vdots \\ c_p
\end{bmatrix}$$
for $c_1, ... c_p$ such that 
$$x = c_1 \mathbf{b}_1 + ... + c_p \mathbf{b}_p$$

\emph{Isomorphism:} a correspondence $x \mapsto [x]_\mathfrak{B}$ which is a one-to-one mapping which preserves linear combinations.

\emph{Dimension:} the dimensions of a nonzero subspace H, denoted dim H, is the number of vectors in any bassis for H

The space $\R^n$ has dimension n so every basis for $\R^n$ consists of n vectors. Similarly, a plane through \textbf{0} is two-dimensional while a line through \textbf{0} is one-dimensional. 

\emph{Rank:} the rank of a matrix A, denoted rank A, is the dimension of the column space of A

Since the pivot columns of A form a basis for Col A, the rank of A is just the number of pivot columns in A.

\emph{The Rank Theorem:} If a matrix A has n columns, then rank A + dim Nul A = n

\emph{The Basis Theorem:} Any linearly independent set of exactly p elements in a p-dimensional subspace of $\R^n$ H is automatically a basis for H.

\subsubsection{Extension of the Invertible Matrix Theorem}
For an $n \times n$ matrix A, the following statements are equivalent to the statement that A is nonsingular:
\begin{enumerate}
	\item The columns of A form a basis of $\R^n$
	\item Col A = $\R^n$
	\item dim Col A = n
	\item rank A = n
	\item Nul A = \{\textbf{0}\}
	\item dim Nul A = 0
\end{enumerate}
\end{document}